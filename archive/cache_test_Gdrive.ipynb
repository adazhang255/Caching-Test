{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A - install packages\n",
    "!pip install --upgrade pip\n",
    "# core libs\n",
    "!pip install vllm lmcache transformers accelerate huggingface_hub\n",
    "!pip install torch\n",
    "\n",
    "# optional: git-lfs if you need to clone on-colab (not recommended for big models)\n",
    "!apt-get update && apt-get install -y git-lfs unzip\n",
    "!git lfs install\n",
    "\n",
    "# Show versions for debugging\n",
    "import importlib, sys\n",
    "for pkg in (\"vllm\",\"lmcache\",\"transformers\",\"accelerate\",\"huggingface_hub\",\"torch\"):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(pkg, m.__version__)\n",
    "    except Exception as e:\n",
    "        print(pkg, \"NOT INSTALLED or failed to import:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B - mount Drive and unzip local copies\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# define paths (edit if you used different folder names)\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Tilli\"\n",
    "MODEL_ZIP = f\"{DRIVE_BASE}/deepseek-coder-6.7B-instruct-GPTQ.zip\"\n",
    "#REPO_ZIP  = f\"{DRIVE_BASE}/myrepo.zip\"\n",
    "\n",
    "# Copy to local ephemeral storage (faster than working on Drive)\n",
    "!cp \"{MODEL_ZIP}\" /content/ || echo \"Model zip not found at {MODEL_ZIP}\"\n",
    "#!cp \"{REPO_ZIP}\" /content/ || echo \"Repo zip not found at {REPO_ZIP}\"\n",
    "\n",
    "# Unzip (overwrite if exists)\n",
    "!unzip -o /content/deepseek-coder-6.7B-instruct-GPTQ.zip -d /content/deepseek_model || true\n",
    "#!unzip -o /content/myrepo.zip -d /content/myrepo || true\n",
    "\n",
    "# List files for verification\n",
    "print(\"Model directory listing:\")\n",
    "!ls -lah /content/deepseek_model | sed -n '1,200p'\n",
    "#print(\"\\nRepo directory listing:\")\n",
    "#!ls -lah /content/myrepo | sed -n '1,200p'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef143d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# The correct model root is directly at /content/deepseek_model/deepseek-coder-6.7B-instruct-GPTQ\n",
    "model_root = \"/content/deepseek_model/deepseek-coder-6.7B-instruct-GPTQ\"\n",
    "\n",
    "print(\"Using model_root:\", model_root)\n",
    "# Diagnostic prints to check path and directory status\n",
    "print(f\"Checking os.path.isdir({model_root}): {os.path.isdir(model_root)}\")\n",
    "assert os.path.isdir(model_root), f\"Model folder not found at {model_root}. Check unzip step.\"\n",
    "\n",
    "# Inspect files to find config / quant metadata\n",
    "print(\"\\nFiles in model root (first 200):\")\n",
    "!ls -lah \"{model_root}\" | sed -n '1,200p'\n",
    "\n",
    "# Try to read a config.json if present (common)\n",
    "cfg_paths = [os.path.join(model_root, f) for f in (\"config.json\", \"model_index.json\", \"config.yaml\")]\n",
    "for cp in cfg_paths:\n",
    "    if os.path.isfile(cp):\n",
    "        print(\"\\nFound config:\", cp)\n",
    "        try:\n",
    "            with open(cp, \"r\", encoding=\"utf-8\") as fh:\n",
    "                raw = fh.read()\n",
    "                preview = (raw[:1000] + \"...\") if len(raw) > 1000 else raw\n",
    "                print(textwrap.fill(preview, width=200))\n",
    "                # quick check for 'mxfp4'\n",
    "                if \"mxfp4\" in raw.lower():\n",
    "                    print(\"\\n*** Warning: 'mxfp4' quant found in config. MXFP4 requires GPU compute capability >= 8.0 (Ampere+).\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read config:\", e)\n",
    "        break\n",
    "else:\n",
    "    print(\"\\nNo config.json found; check model files for quant format (e.g. .bin, .gguf).\")\n",
    "\n",
    "# Check GPU capability programmatically\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    cc = torch.cuda.get_device_capability(0)\n",
    "    print(\"\\nCUDA device capability:\", cc)\n",
    "    if isinstance(cc, tuple) and (cc[0] >= 8):\n",
    "        print(\"GPU seems Ampere+ (compatible with MXFP4).\")\n",
    "    else:\n",
    "        print(\"GPU compute capability < 8.0. If model uses MXFP4 you will see an error loading it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39971381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now instantiate vLLM with LMCache. We will try to use AWQ quantization.\n",
    "print(\"\\nAttempting to create LM with vLLM + LMCache (this may fail if format unsupported)\")\n",
    "try:\n",
    "    import lmcache\n",
    "    from vllm import LLM, SamplingParams\n",
    "\n",
    "    # 2. Define the LMCache configuration\n",
    "    kv_cache_config = {\n",
    "        \"kv_connector\": \"LMCacheConnectorV1\",\n",
    "        \"kv_role\": \"kv_both\"\n",
    "    }\n",
    "\n",
    "    # 3. Initialize the vLLM engine with LMCache\n",
    "    print(\"Loading model...\")\n",
    "    llm = LLM(\n",
    "        model=model_root,\n",
    "        quantization=\"GPTQ\",\n",
    "        kv_transfer_config=kv_cache_config,\n",
    "        dtype=\"auto\" \n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    " \n",
    "except Exception as e:\n",
    "    print(\"Failed to instantiate vLLM LLM():\\n\", e)\n",
    "    raise\n",
    "\n",
    "\n",
    "# 4. Define sampling parameters\n",
    "from datetime import time\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=100)\n",
    "\n",
    "# 5. Define prompts to test caching\n",
    "prompts = [\n",
    "    (\"What is the capital of France?\"),\n",
    "\n",
    "    (\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "# --- Run Generations ---\n",
    "\n",
    "# Run the first prompt (will be slower and populate the cache)\n",
    "print(\"\\n--- Running first prompt (populating cache) ---\")\n",
    "start_time = time.time()\n",
    "outputs = llm.generate([prompts[0]], sampling_params)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {output.prompt}\")\n",
    "    print(f\"Generated: {generated_text}\\n\")\n",
    "\n",
    "\n",
    "# Run the second prompt (will be faster due to cached prefix)\n",
    "print(\"\\n--- Running second prompt (using cache) ---\")\n",
    "start_time = time.time()\n",
    "outputs = llm.generate([prompts[1]], sampling_params)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {output.prompt}\")\n",
    "    print(f\"Generated: {generated_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
