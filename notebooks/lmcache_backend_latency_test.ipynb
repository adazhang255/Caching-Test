{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMCache Backend Latency Test (GPU + Disk)\n",
    "\n",
    "This Colab-friendly notebook starts an LMCache controller, configures a local Disk tier,\n",
    "runs a small vLLM model, and measures latency for repeated prompts while steering\n",
    "placement between GPU (hot) and Disk (warm) using perplexity/time-variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef59b2e",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "\n",
    "- Define repo, config, log, and disk paths.\n",
    "- Install dependencies and optionally unzip your repo into /content/src.\n",
    "- Update LMCache config for GPU + Disk (no CPU/Remote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef40d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [1] Setting up environment and paths ===\n",
      "Paths set: /content/src /content/src/config.yaml /content/src/controller.log /content/lmcache_warm\n",
      "=== [1] Setup paths DONE ===\n"
     ]
    }
   ],
   "source": [
    "print('=== [1] Setting up environment and paths ===')\n",
    "# Environment and paths\n",
    "import os, sys, textwrap\n",
    "REPO_DIR = '/content/src'\n",
    "CONFIG   = f'{REPO_DIR}/config.yaml'\n",
    "LOGFILE  = f'{REPO_DIR}/controller.log'\n",
    "DISK_DIR = '/content/lmcache_warm'\n",
    "os.makedirs(REPO_DIR, exist_ok=True)\n",
    "os.makedirs(DISK_DIR, exist_ok=True)\n",
    "print('Paths set:', REPO_DIR, CONFIG, LOGFILE, DISK_DIR)\n",
    "print('=== [1] Setup paths DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b517b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [2] Installing dependencies and checking GPU ===\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hName: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mName: vllm\n",
      "Version: 0.11.2\n",
      "Summary: A high-throughput and memory-efficient inference and serving engine for LLMs\n",
      "Home-page: https://github.com/vllm-project/vllm\n",
      "Author: vLLM Team\n",
      "Author-email: \n",
      "License-Expression: Apache-2.0\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: aiohttp, anthropic, blake3, cachetools, cbor2, cloudpickle, compressed-tensors, depyf, diskcache, einops, fastapi, filelock, flashinfer-python, gguf, lark, llguidance, lm-format-enforcer, mistral_common, model-hosting-container-standards, msgspec, ninja, numba, numpy, openai, openai-harmony, opencv-python-headless, outlines_core, partial-json-parser, pillow, prometheus-fastapi-instrumentator, prometheus_client, protobuf, psutil, py-cpuinfo, pybase64, pydantic, python-json-logger, pyyaml, pyzmq, ray, regex, requests, scipy, sentencepiece, setproctitle, setuptools, six, tiktoken, tokenizers, torch, torchaudio, torchvision, tqdm, transformers, typing_extensions, watchfiles, xformers, xgrammar\n",
      "Required-by: \n",
      "GPU available: True\n",
      "lmcache.v1.api_server import OK\n",
      "=== [2] Dependencies/GPU check DONE ===\n"
     ]
    }
   ],
   "source": [
    "print('=== [2] Installing dependencies and checking GPU ===')\n",
    "# Install dependencies (rerun on fresh runtimes)\n",
    "!pip -q install -U pip\n",
    "!pip -q install transformers jedi pydantic\n",
    "!pip show transformers\n",
    "# vLLM provides the LLM runtime; install if not present\n",
    "!pip -q install vllm\n",
    "!pip show vllm\n",
    "# LMCache Python package (provides lmcache.v1.api_server)\n",
    "!pip -q install lmcache\n",
    "import subprocess, importlib\n",
    "gpu_rc = subprocess.run(['bash','-lc','nvidia-smi'], capture_output=True)\n",
    "gpu_available = (gpu_rc.returncode == 0)\n",
    "print('GPU available:', gpu_available)\n",
    "try:\n",
    "    importlib.import_module('lmcache.v1.api_server')\n",
    "    print('lmcache.v1.api_server import OK')\n",
    "except ImportError as e:\n",
    "    raise RuntimeError('lmcache.v1.api_server not available; ensure pip install lmcache succeeded.') from e\n",
    "print('=== [2] Dependencies/GPU check DONE ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b83c10",
   "metadata": {},
   "source": [
    "## Unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ed2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [3] Checking for uploaded repo (src.zip) ===\n",
      "total 8\n",
      "drwxr-xr-x 2 root root 4096 Nov 21 14:24 .\n",
      "drwxr-xr-x 1 root root 4096 Nov 21 14:24 ..\n",
      "=== [3] Repo presence check DONE ===\n"
     ]
    }
   ],
   "source": [
    "print('=== [3] Checking for uploaded repo (src.zip) ===')\n",
    "# If you uploaded src.zip, unzip it to /content/src\n",
    "from pathlib import Path\n",
    "if Path('src.zip').exists():\n",
    "    !unzip -o src.zip -d /content > /dev/null\n",
    "!ls -la /content/src || true\n",
    "print('=== [3] Repo presence check DONE ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01afd2d",
   "metadata": {},
   "source": [
    "## 3. Check GPU status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    cc = torch.cuda.get_device_capability(0)\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "\n",
    "    print(f\"GPU: {device_name}\")\n",
    "    print(f\"Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "    print(f\"Memory: {mem_gb:.1f} GB\")\n",
    "\n",
    "    if cc[0] >= 7 and cc[1] >= 5:\n",
    "        print(\"✓ T4 or better - W4A16 quantization supported\")\n",
    "    else:\n",
    "        print(\"⚠️  GPU may not support quantization kernels\")\n",
    "else:\n",
    "    print(\"✗ No CUDA GPU detected!\")\n",
    "    raise RuntimeError(\"GPU required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab658a29",
   "metadata": {},
   "source": [
    "## 2) Initialize Model and Cache\n",
    "\n",
    "- Initialize a small model in vLLM (Gemma 270M).\n",
    "- Bind LMCache controller + MultiTierCache manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315c1467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f74e80b375403ca9d36899e0a1b1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Login was **successful**!\n",
      "Logged in as **redbeardthedetective** (user ID: 6907daf9831f263751c64fb3)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accessTokenRole'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2520821458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Login was **successful**!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Logged in as **{user_info['name']}** (user ID: {user_info['id']})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Permissions: {user_info['auth']['accessTokenRole']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=== [4] Setup complete. You can now run the LMCache backend server. ==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accessTokenRole'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "# This will prompt you to paste your token\n",
    "notebook_login()\n",
    "user_info = whoami()\n",
    "\n",
    "# If whoami() runs without raising an exception, the login was successful.\n",
    "print(\"✅ Login was **successful**!\")\n",
    "print(f\"Logged in as **{user_info['name']}** (user ID: {user_info['id']})\")\n",
    "print(f\"Permissions: {user_info['auth']['accessTokenRole']}\")\n",
    "print('=== [4] Setup complete. You can now run the LMCache backend server. ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ee62d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "✓ Model already cached at /content/models/gemma-3-270m-it\n",
      "total 28K\n",
      "-rw-r--r-- 1 root root 28K Nov 21 14:29 README.md\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "model_id = \"google/gemma-3-270m-it\"\n",
    "local_dir = \"/content/models/gemma-3-270m-it\"\n",
    "\n",
    "if os.path.exists(local_dir):\n",
    "    print(f\"✓ Model already cached at {local_dir}\")\n",
    "else:\n",
    "    print(f\"Downloading {model_id}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        local_dir=local_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(f\"✓ Downloaded to {local_dir}\")\n",
    "\n",
    "# Verify download\n",
    "!ls -lh {local_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fb522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "Loading model with vLLM + LMCache...\n",
      "(This takes 30-60s on T4)\n",
      "\n",
      "INFO 11-21 16:02:50 [utils.py:253] non-default args: {'max_model_len': 2048, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'enforce_eager': True, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='e2cf25d7-8247-46ff-8c9b-50661c32f6ef', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': '/content/models/gemma-3-270m-it'}\n",
      "\n",
      "✗ Model loading failed: 1 validation error for ModelConfig\n",
      "  Value error, Invalid repository ID or local directory specified: '/content/models/gemma-3-270m-it'.\n",
      "Please verify the following requirements:\n",
      "1. Provide a valid Hugging Face repository ID.\n",
      "2. Specify a local directory that contains a recognized configuration file.\n",
      "   - For Hugging Face models: ensure the presence of a 'config.json'.\n",
      "   - For Mistral models: ensure the presence of a 'params.json'.\n",
      "3. For GGUF: pass the local path of the GGUF checkpoint.\n",
      "   Loading GGUF from a remote repo directly is not yet supported.\n",
      " [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ModelConfig\n  Value error, Invalid repository ID or local directory specified: '/content/models/gemma-3-270m-it'.\nPlease verify the following requirements:\n1. Provide a valid Hugging Face repository ID.\n2. Specify a local directory that contains a recognized configuration file.\n   - For Hugging Face models: ensure the presence of a 'config.json'.\n   - For Mistral models: ensure the presence of a 'params.json'.\n3. For GGUF: pass the local path of the GGUF checkpoint.\n   Loading GGUF from a remote repo directly is not yet supported.\n [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2500192260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     llm = LLM(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mengine_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUsageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLM_CLASS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Create the engine configs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_engine_config\u001b[0;34m(self, usage_context, headless)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             )\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm_encoder_tp_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         return ModelConfig(\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mhf_config_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_config_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_dataclasses.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__dataclass_self__, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__dataclass_self__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgsKwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0m__init__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{cls.__qualname__}.__init__'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ModelConfig\n  Value error, Invalid repository ID or local directory specified: '/content/models/gemma-3-270m-it'.\nPlease verify the following requirements:\n1. Provide a valid Hugging Face repository ID.\n2. Specify a local directory that contains a recognized configuration file.\n   - For Hugging Face models: ensure the presence of a 'config.json'.\n   - For Mistral models: ensure the presence of a 'params.json'.\n3. For GGUF: pass the local path of the GGUF checkpoint.\n   Loading GGUF from a remote repo directly is not yet supported.\n [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"/content/models/gemma-3-270m-it\"\n",
    "\n",
    "print(\"Loading model with vLLM + LMCache...\")\n",
    "print(\"(This takes 30-60s on T4)\\n\")\n",
    "\n",
    "# LMCache configuration\n",
    "kv_cache_config = {\n",
    "    \"kv_connector\": \"LMCacheConnectorV1\",\n",
    "    \"kv_role\": \"kv_both\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        dtype=\"auto\",\n",
    "        gpu_memory_utilization=0.8,\n",
    "        max_model_len=2048,\n",
    "        kv_transfer_config=kv_cache_config,\n",
    "        enforce_eager=True  # Disable CUDA graphs for compatibility\n",
    "    )\n",
    "    print(\"\\n✓ Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Model loading failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ebf0b",
   "metadata": {},
   "source": [
    "## 3) Start LMCache Controller\n",
    "\n",
    "- Tail controller logs for quick diagnostics.\n",
    "- Start via `src.notebook_bootstrap.start_controller()` (wraps the lmcache CLI for Colab).\n",
    "- Verify health on http://127.0.0.1:9000/health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39135f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Wiring LMCacheController + MultiTierCache ===')\n",
    "# Import LMCache client + manager\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "from src.cache_controller import LMCacheController\n",
    "from src.tiered_caching import MultiTierCache\n",
    "controller = LMCacheController(host='127.0.0.1', port=9000, model='google/gemma-3-270m-it')\n",
    "cache_manager = MultiTierCache(controller)\n",
    "cache_manager.set_llm(llm)\n",
    "print('Cache manager ready')\n",
    "print('=== [9] LMCacheController wiring DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [5] Helper print_last_lines() defined ===\n"
     ]
    }
   ],
   "source": [
    "# Helper to tail last N lines of a file\n",
    "from collections import deque\n",
    "def print_last_lines(path: str, n: int = 80):\n",
    "    if not os.path.exists(path):\n",
    "        print('No log at', path) \n",
    "        return\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            last = deque(f, maxlen=n)\n",
    "        print(f'--- Last {len(last)} lines of {path} ---')\n",
    "        for b in last:\n",
    "            print(b.decode('utf-8', errors='replace').rstrip())\n",
    "    except Exception as e:\n",
    "        print('log read error:', e)\n",
    "print('=== [5] Helper print_last_lines() defined ===')\n",
    "print('=== [5] Log helper cell DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [6] Starting LMCache controller (script or CLI) ===\n",
      "Starting controller via lmcache_controller CLI\n",
      "--- Last 4 lines of /content/src/controller.log ---\n",
      "usage: lmcache_controller [-h] [--host HOST] [--port PORT]\n",
      "                          [--monitor-ports MONITOR_PORTS]\n",
      "                          [--monitor-port MONITOR_PORT]\n",
      "lmcache_controller: error: unrecognized arguments: --config /content/src/config.yaml\n"
     ]
    }
   ],
   "source": [
    "print('=== [6] Starting LMCache controller via cache_controller ===')\n",
    "import time\n",
    "os.environ['LMCACHE_CONFIG_FILE'] = CONFIG\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "try:\n",
    "    pid = controller.start_controller(\n",
    "        config_path=CONFIG,\n",
    "        host='127.0.0.1',\n",
    "        port=9000,\n",
    "        log_path=LOGFILE,\n",
    "    )\n",
    "    print('Controller PID:', pid)\n",
    "    print('Controller running?', controller.controller_running())\n",
    "except Exception as e:\n",
    "    print('Error starting controller via cache_controller:', e)\n",
    "time.sleep(1.0)\n",
    "print_last_lines(LOGFILE, 60)\n",
    "print('=== [6] Controller start cell DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed85d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [7] Health check: LMCache controller /healthz ===\n",
      "Controller not reachable; check logs above\n"
     ]
    }
   ],
   "source": [
    "print('=== [7] Health check: LMCache controller /health ===')\n",
    "# Health check loop\n",
    "import requests, time\n",
    "ok=False\n",
    "for _ in range(10):\n",
    "    r = controller.health()\n",
    "    print('health:', r)\n",
    "    ok=True\n",
    "    break\n",
    "    if not r.ok:\n",
    "        time.sleep(0.5)\n",
    "if not ok:\n",
    "    print('Error: Controller not reachable; check logs above')\n",
    "print('=== [7] Health check cell DONE ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd3fe8",
   "metadata": {},
   "source": [
    "## 4) Latency Tests (GPU vs Disk)\n",
    "\n",
    "- Use perplexity and time-variance to steer placement.\n",
    "- Run twice to observe warm-cache speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to run a latency test with desired perplexity/time_variance\n",
    "import time as _t\n",
    "def latency_run(label: str, prompt: str, perplexity: float, time_variance: float, max_tokens: int = 64):\n",
    "    print('Controller running?', controller.controller_running())\n",
    "    \n",
    "    meta = {'perplexity': perplexity, 'time_variance': time_variance}\n",
    "    sp_local = SamplingParams(max_tokens=max_tokens, temperature=0.0)\n",
    "    print(f'=== [10] latency_run start: {label} | perplexity={perplexity}, time_variance={time_variance}, max_tokens={max_tokens} ===')\n",
    "    print('Prompt length (chars):', len(prompt))\n",
    "    # Quick sanity on controller + LLM\n",
    "    try:\n",
    "        h = controller.health()\n",
    "        print('controller.health():', h)\n",
    "    except Exception as e:\n",
    "        print('controller.health() failed:', repr(e))\n",
    "    print('llm type:', type(llm))\n",
    "    # First run\n",
    "    t0 = _t.time()\n",
    "    try:\n",
    "        _ = cache_manager.generate_and_manage(prompt, sp_local, metadata=meta.copy())\n",
    "    except Exception as e:\n",
    "        t_err = _t.time()\n",
    "        print(f'ERROR during first generate_and_manage for {label}:', repr(e))\n",
    "        print('Elapsed before error (s):', t_err - t0)\n",
    "        raise\n",
    "    t1 = _t.time()\n",
    "    tokens = controller.tokenize(prompt)\n",
    "    layout = controller.lookup(tokens)\n",
    "    print('layout after first run:', layout)\n",
    "    b1 = layout.get('lmcache_default_instance', [None])[0]\n",
    "    # Second run\n",
    "    t2 = _t.time()\n",
    "    try:\n",
    "        _ = cache_manager.generate_and_manage(prompt, sp_local, metadata=meta.copy())\n",
    "    except Exception as e:\n",
    "        t_err2 = _t.time()\n",
    "        print(f'ERROR during second generate_and_manage for {label}:', repr(e))\n",
    "        print('Elapsed before error (s):', t_err2 - t2)\n",
    "        raise\n",
    "    t3 = _t.time()\n",
    "    layout2 = controller.lookup(tokens)\n",
    "    print('layout after second run:', layout2)\n",
    "    b2 = layout2.get('lmcache_default_instance', [None])[0]\n",
    "    print(f'[{label}] Run1 backend={b1} latency={t1-t0:.3f}s | Run2 backend={b2} latency={t3-t2:.3f}s')\n",
    "    return {'label': label, 'b1': b1, 't1': t1-t0, 'b2': b2, 't2': t3-t2}\n",
    "print('=== [10] latency_run helper definition DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e65699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-preferred test (hot): low perplexity / low variance\n",
    "print('--- GPU test: checking controller health before latency_run ---')\n",
    "try:\n",
    "    print('pre-run controller.health():', controller.health())\n",
    "except Exception as e:\n",
    "    print('pre-run controller.health() failed:', repr(e))\n",
    "print('--- GPU test: calling latency_run ---')\n",
    "res_gpu = latency_run('GPU', 'Hello GPU test ' * 128, perplexity=5.0, time_variance=0.1, max_tokens=64)\n",
    "print('GPU test result dict:', res_gpu)\n",
    "print('=== [10a] GPU latency_run invocation DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disk-preferred test (warm): high perplexity / high variance\n",
    "res_disk = latency_run('Disk', 'Move to disk test ' * 512, perplexity=100.0, time_variance=0.9, max_tokens=32)\n",
    "res_disk\n",
    "print('=== [10b] Disk latency_run invocation DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e27d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test: tweak to steer placement\n",
    "custom_prompt = 'Custom backend placement test ' * 256\n",
    "res_custom = latency_run('Custom', custom_prompt, perplexity=30.0, time_variance=0.5, max_tokens=48)\n",
    "res_custom\n",
    "print('=== [10c] Custom latency_run invocation DONE ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7fdce",
   "metadata": {},
   "source": [
    "## 5) Summary & Inspect Disk\n",
    "\n",
    "- Review timings and backends.\n",
    "- List a few files from the Disk tier directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== [11] Summary and Disk inspection ===')\n",
    "# Summary\n",
    "from pprint import pprint\n",
    "pprint({'GPU': res_gpu, 'Disk': res_disk, 'Custom': res_custom})\n",
    "print('Disk dir contents (first few files):')\n",
    "!find /content/lmcache_warm -type f | head -n 10\n",
    "print('=== [11] Summary cell DONE ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946a21b",
   "metadata": {},
   "source": [
    "## 6) TTFT Batch Benchmarks (Long Contexts)\n",
    "\n",
    "Measure approximate TTFT by generating only 1 token.\n",
    "Runs multiple prompts with long shared prefixes, comparing first vs second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== [12] Running TTFT batch benchmarks (long-context prompts) ===')\n",
    "import time as _t\n",
    "from pprint import pprint\n",
    "\n",
    "def make_long_context(repeats: int = 2048) -> str:\n",
    "    base = 'In a distant future, advanced neural architectures collaborate with humans to solve complex problems. '\n",
    "    return base * (repeats // 8)\n",
    "\n",
    "base_ctx = make_long_context(2048)\n",
    "prompts = [f'{base_ctx} Task {i}: Write a concise plan.' for i in range(5)]\n",
    "sp_ttft = SamplingParams(max_tokens=1, temperature=0.0)\n",
    "\n",
    "results = []\n",
    "for i, p in enumerate(prompts):\n",
    "    t0 = _t.time()\n",
    "    _ = cache_manager.generate_and_manage(p, sp_ttft, metadata={'perplexity': 12.0, 'time_variance': 0.2})\n",
    "    t1 = _t.time()\n",
    "    tokens = controller.tokenize(p)\n",
    "    layout = controller.lookup(tokens)\n",
    "    b1 = layout.get('lmcache_default_instance', [None])[0]\n",
    "\n",
    "    t2 = _t.time()\n",
    "    _ = cache_manager.generate_and_manage(p, sp_ttft, metadata={'perplexity': 12.0, 'time_variance': 0.2})\n",
    "    t3 = _t.time()\n",
    "    layout2 = controller.lookup(tokens)\n",
    "    b2 = layout2.get('lmcache_default_instance', [None])[0]\n",
    "\n",
    "    results.append({'idx': i, 'ttft_first': t1-t0, 'ttft_second': t3-t2, 'backend_first': b1, 'backend_second': b2})\n",
    "\n",
    "print('TTFT results (seconds):')\n",
    "pprint(results)\n",
    "print('Avg first:', sum(r['ttft_first'] for r in results)/len(results))\n",
    "print('Avg second:', sum(r['ttft_second'] for r in results)/len(results))\n",
    "print('=== [12] TTFT benchmarks cell DONE ===')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
